---
title: Transformations and Nonparametric Methods
header-includes: \usepackage{color}
output:
  pdf_document:
    fig_caption: no
  html_document: default
---
<style>
table, th, td { text-align:right; }
th, td {padding: 10px;}
</style>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
whichcomp <- strsplit(getwd(),"/")[[1]][3]
load(paste0("c:/users/", whichcomp, "/Dropbox/teaching/Resma3/Resma3.RData"))
library(knitr)
opts_chunk$set(fig.width=6, fig.align = "center", 
      out.width = "70%", warning=FALSE, message=FALSE)
library(ggplot2)
library(grid)
```
`r hl()$basefontsize()`

Many classic methods of analysis (t tests, F tests etc) have an assumption of data that comes from a normal distribution. What can we do if we don't have that? There are two ways to proceed:

## Transformations

A *data transformation* is any mathematical function applied to the data. Sometimes such a transformation can be used to make distribution more normal:

#### **Example**: Body and Brain Weight

Consider the data set *brainsize*, which has the weights of the body (in kg) and of the brain (in gram) of 62 mammals:

```{r}
brainsize
```

Let's say we want to find a $95\%$ confidence interval for the mean body weight. If we want to use the *t.test* method we need to check normality:

```{r}
qplot(data=brainsize, sample=body.wt.kg) +
  stat_qq() + stat_qq_line()
```

and this is clearly non-normal. However

```{r}
qplot(data=brainsize, sample=log(body.wt.kg)) +
  stat_qq() + stat_qq_line()
```

shows that $\log$(body.wt.kg) is indeed normally distributed. So now

```{r nonpar-ttest}
t.test(log(brainsize$body.wt.kg))$conf.int
```

but this is confidence interval for $\log$(body.wt.kg), we want one for body.wt.kg. 

$$
\begin{aligned}
&0.567 \le \log \mu \le 2.163 \text{  } \Leftrightarrow \\
&\exp (0.567) \le \mu \le \exp (2.163)  \text{  } \Leftrightarrow \\
&1.763 \le \mu \le 8.697
\end{aligned}
$$

Here we used a log transform. In principle any function might work, and there is even a way to pick the best from a list: In 1964 Box and Cox suggested a family of transformations of the form

$$
T_{\lambda }(x)=\left\{ 
\begin{array}{cc}
\frac{x^{\lambda }-1}{\lambda } & \lambda \neq 0 \\ 
\log x & \lambda =0
\end{array}
\right.
$$
Notice that this is continuous in $\lambda$: $\lim_{\lambda \rightarrow 0} T_\lambda (x)=T_0(x)$ and it includes $1/x, \sqrt{x}, x^k$ etc. To pick the best use 

```{r}
library(MASS)
fit <- lm(brainsize$body.wt.kg~rep(1, 62))
boxcox(fit, lambda=seq(-0.25, 0.25, 0.01))
```

the vertical lines give a confidence interval for $\lambda$, and any value inside the interval is acceptable. It seems that for our data $\lambda=0$ or the log transform is indeed approriate.

Note that the *boxcox* command requires a fit object generated by *lm* or *aov* but can also be used for single vectors as above.


## Nonparametric Methods


A different way to proceed is to use a method that does not require a normal distribution. These are so called *nonparametric methods*. A number of them have been developed over the years as alternatives to the standard normal based methods.

In general nonparametric methods are based on the *ranks* of the observations and focus on the median instead of the mean.

### Alternative to 1 Sample t

#### **Example**: Euro Coins

The data were collected by Herman Callaert at Hasselt University in Belgium. The euro coins were borrowed at a local bank. Two assistants, Sofie Bogaerts and Saskia Litiere weighted the coins one by one, in laboratory conditions on a weighing scale of the type Sartorius BP 310.

Say we are told that a one euro coin is supposed to weigh 7.5 grams. Does the data in support that claim?


```{r}
head(euros)
ggplot(euros, aes(y=Weight, x=rep("", 2000))) +
  geom_boxplot() +
  labs(x="")
```

The boxplot of Weight shows severe outliers, so the usual 1 sample t test won't work. Unfortunately the log transformation does not work here either. This is not a surprise, by the way, because the outliers are on both sides of the box.

The name of the test that works here is **Wilcoxon Signed Rank Test**.
 
The details are
```{r}
wilcox.test(euros$Weight, mu=7.5)  
```

1) Parameter of interest: 1 median  
2) Method of analysis: Wilcoxon Signed Rank test   
3) Assumptions of Method: **none**  
4) $\alpha$ = 0.05   
5) Null hypothesis H~0~: M=7.5 (median weight is 7.5 grams)   
6) Alternative hypothesis H~a~: M $\ne$ 7.5 (median weight is not 7.5 grams)   
7) p value = 0.000  
8) 0.000<0.05, so we reject H~0~, it seems the median weight is not 7.5 grams.

Actually, in this data set we could still have used the usual 1-sample t test (also with a p-value of 0.000) because we have a very large sample (n=2000), but in general it is never clear exactly how large a sample needs to be to "overcome" some outliers, so these non-parametric tests are always a safe alternative. 

### Why not always use the non-parametric test?

If using the t test sometimes is wrong but the Wilcoxon Rank Sum test always works, why not just always use this test and be safe? The answer is that the t test has a larger power: 
 
```{r cache=TRUE}
mu <- seq(0, 1.5, length=100)
pw <- matrix(0, 100, 2)
colnames(pw) <- c("t test", "Wilcoxon")
B <- 10000
for(i in 1:100) {
  for(j in 1:B) {
    x <- rnorm(10, mu[i])
    pval <- t.test(x, mu=0)$p.value
    if(pval<0.05) pw[i, 1] <- pw[i, 1]+1
    pval <- wilcox.test(x, mu=0)$p.value
    if(pval<0.05) pw[i, 2] <- pw[i, 2]+1
  }
}
pw <- 100*pw/B
```
 
```{r}
df <- data.frame(
        Mean=c(mu, mu),
        Power=c(pw[, 1], pw[, 2]),
        Method=rep(c("t test", "Wilcoxon"), each=100))
ggplot(df, aes(Mean, Power, color=Method)) +
  geom_line()
```
 

In real life the  power of the nonparametric tests is often almost as high as the power of the standard tests, so they should always be used if there is a question about the normal assumption. 


If we wanted a 90% confidence interval for median we could use

```{r}
wilcox.test(euros$Weight, 
            conf.int=TRUE, 
            conf.level=0.9)$conf.int  
```

### Alternative to two sample t

just as with the t.test command, the wilcox.test command can also be used to compare the means of two populations:

#### **Example**: Euro Coins

Are the means of the weights of the coins in rolls 7 and 8 different?

```{r}
x <- euros$Weight[euros$Roll==7]
y <- euros$Weight[euros$Roll==8]
wilcox.test(x, y)
```



### Alternative to ANOVA

#### **Example**: Euro Coins

Say we want to know whether the coins in the 8 different rolls have the same average weight. The non-parametric alternative to the oneway ANOVA is  the **Kruskal-Wallis test:**

```{r}
kruskal.test(Weight~factor(Roll), data=euros) 
```

1) Parameters of interest: **medians**  
2) Method of analysis: Kruskal-Wallis  
3) Assumptions of Method: **none**  
4) $\alpha$ = 0.05  
5) Null hypothesis H~0~: M~1~=..=M~8~ (group **medians** are the same)  
6) Alternative hypothesis H~a~: M~i~ $\ne$ M~j~ for some i, j(group medians are not the same)  
7) p value = 0.00  
8) 0.00 < 0.05, so we reject H~0~, it seems the group medians are not the same  

#### **Example**: Cultural Differences in Equipment Use

A US company manufactures equipment that is used in the production of semiconductors. The firm is considering a costly redesign that will improve the performance of its equipment. The performance is characterized as mean time between failures (MTBF). Most of the companies customers are in the USA, Europe and Japan, and there is anectotal evidence that the Japanese customers typically get better performance from the users in the USA and Europe.


```{r}
head(culture)
table(culture$Country)
ggplot(culture, aes(Country, MTBF)) +
  geom_boxplot()
```
  
There is a problem with the normal assumption. We can try to fix this with the log transform, but again this does not work.

Because none of the transformations worked we will use the non-parametric Kruskall-Wallis test:

```{r}
kruskal.test(MTBF~factor(Country), data=culture)
```

1) Parameters of interest: medians  
2) Method of analysis: Kruskal-Wallis  
3) Assumptions of Method: **none**  
4) $\alpha = 0.05$   
5) Null hypothesis H~0~: M~1~ = M~2~ = M~3~ (group medians are the same)  
6) Alternative hypothesis H~a~: M~i~ $\ne$ M~j~ for some i, j (group medians are not the same)  
7) p value = 0.001  
8) 0.001 < 0.05, so we reject H~0~, it seems the group medians are not the same, the MTBF is different in diffferent countries 

If we had just done the ANOVA Country would not have been stat. significant (p-value = 0.098) but if you remember to check the normal plot you will see that there is a problem with this analysis.
