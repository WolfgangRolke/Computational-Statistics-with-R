---
header-includes: \usepackage{color}
                 \usepackage{float}
output:
  html_document: default
  pdf_document:
    fig_caption: no
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
source("R/setup.rmd.R", local=TRUE)
setup.rmd(local.env=environment())
```
`r hl()$basefontsize()`
`r hl()$style()`

## Simulation

### Basic Idea

We already used simulation in a couple of situations. In this section we will take a closer look.

#### **Example**: Binomial proportion

Let's start with a simple example: say we flip a coin 100 times and get 62 heads. Is this a fair coin?

Of course we discussed this problem before and there is a standard solution: we want to test

$$
H_0: \pi=0.5 \text{ vs. } H_a:\pi \ne 0.5
$$
the test is done with 

```{r}
binom.test(62, 100)
```

and with a p value of `r round(binom.test(62, 100)$p.value, 4)` we would reject the null hypothesis at the 5% level.

But let's assume for the moment we do not know this solution. What could we do?

we can simulate flipping a fair coin and counting the number of heads with

```{r}
smpl <- sample(c("Heads", "Tails"), size=100, replace=TRUE)
x <- sum(smpl=="Heads")
x
```

Now we can do this many times:

```{r}
B <- 1e4
x <- rep(0, B)
for(i in 1:B) {
  smpl <- sample(c("Heads", "Tails"), 
                 size=100, replace=TRUE)
  x[i] <- sum(smpl=="Heads")  
}
table(x)
```

and we can find the p-value with

```{r}
(sum(x<=38)+sum(x>=62))/B
```

So here we did the test via simulation, without any theory!

### Simulation Error

Everytime we run a simulation, the computer generates different data and so the answer comes out a bit different as well. But how different?

At least in one case there is a simple way to estimate this simulation error. In the example above, each simulation run is essentially a Bernoulli trial ($x\le 38, x\ge 62$ or not). An $95\% confidence interval for the true proportion is given by

$$
\hat{\pi} \pm 2\sqrt{\hat{\pi}(1-\hat{\pi})/n}
$$

#### **Example**:   Binomial proportion, continued


```{r}
0.02 + c(-2, 2)*sqrt(0.02*0.98/1e4)
```

so the true p value is between 0.017 and 0.023, in either case < 0.05 and we would reject the null.

`r hl()$hr()`

#### **Example** Exponential rate

the sample below is assumed to come from an exponential distribution rate $\lambda$. We wish to test

$$
H_0: \lambda=1 \text{ vs. } H_a:\lambda > 1
$$

```{r echo=FALSE}
set.seed(111)
exp.data <- sort(round(rexp(50, 1.5), 2))
```

```{r}
exp.data
```

here is a solution via simulation. We know from theory that the mle of $\lambda$ is $1/\hat{X}$, so


```{r}
B <- 1e4
sim.data <- rep(0, B)
for(i in 1:B) {
  sim.data[i] <- 
    1/mean(rexp(length(exp.data), 1))
}
sum(sim.data>1/mean(exp.data))/B
```

#### **Example** Normal mean

below we have data from a normal distribution and we want to test

$$
H_0: \mu=10 \text{ vs. } H_a:\mu > 10
$$


```{r echo=FALSE}
set.seed(112)
norm.data <- sort(round(rnorm(20, 10), 2))
```

```{r}
norm.data
```

Again we want to use simulation and we can use the sample mean as our test statistic, but here we have an additional problem: we will of course generate data from a normal distribution with mean 10, but what should we use as the standard deviation? It is not defined by the null hypothesis. 

There is an obvious answer: use the sample standard deviation. It is not clear however if that is indeed legitimate.

```{r}
B <- 1e4
n <- length(norm.data)
p <- sd(norm.data)
sim.data <- rep(0, B)
for(i in 1:B) {
  sim.data[i] <- 
    mean(rnorm(n, 10, p))
}
sum(sim.data>mean(norm.data))/B
```

how does this compare to the standard answer?

```{r}
t.test(norm.data, mu=10, alternative = "greater")$p.value
```

pretty close, certaily within simulation error.


sometimes one varies the standard deviation a bit in the simulation step. R does not have a method for finding confidence intervals for variances, but here is how to find them:  

```{r}
v <- var(norm.data)
lower <- v*19/qchisq(0.05/2, 19, 
                             lower.tail = FALSE)
upper <- v*19/qchisq(1-0.05/2, 19, 
                             lower.tail = FALSE)
sqrt(c(lower = lower, upper = upper))
```
 
and so we can run the simulation also this way:
 

```{r}
B <- 1e4
n <- length(norm.data)
sim.data <- rep(0, B)
for(i in 1:B) {
  sim.data[i] <- 
    mean(rnorm(n, 10, runif(1, 0.836, 1.605)))
}
sum(sim.data>mean(norm.data))/B
```

In essence this has a bit of a Bayesian flavour, we just introduced a prior for $\sigma$!

###  Permutation Tests

There is a class of methods essentially built on simulation. Here is an 

#### **Example**: Equal means

below are two data sets. Do they came from the same type of distribution but with different means? 

So there is a distribution $F$, and we can assume without loss of generality that $E[X_F=0$]. There are $\mu_1$ and $\mu_2$ such that  

$$
\begin{aligned}
&X_1-\mu_1, .., X_n -\mu_1 \sim F \\
&Y_1-\mu_2, .., Y_m -\mu_2 \sim F \\
\end{aligned}
$$

and we have the hypotheses

$$
H_0: \mu_1=\mu_2 \text{ vs. } H_a:\mu_1 \ne \mu_2
$$


```{r echo=FALSE}
set.seed(1123)
norm1.data <- sort(round(rnorm(20, 20, 3), 1))
norm2.data <- sort(round(rnorm(30, 20, 3), 1))
```

```{r}
norm1.data
norm2.data
df <- data.frame(
  x=c(norm1.data, norm2.data),
  y=rep(c("1", "2"), c(20, 30)))
ggplot(df, aes(y, x)) +
  geom_boxplot()
```

a reasonable test statistics would be

$$
T=\frac{\bar{X}-\bar{Y}}{\sqrt{[(n-1)s_X^2+(m-1)s_Y^2]/(n+m-2)}}
$$

because under the null $E[\bar{X}-\bar{Y}]=0$ and the denominator is the usual estimatator of the standard deviation (called the *pooled* standard deviation).

```{r}
x <- norm1.data
y <- norm2.data
T0 <- (mean(x)-mean(y))/sqrt((19*var(x)+29*var(y))/49)
T0
```



Now the idea is as follows: under the null hypothesis all the X's and Y's are an independent sample from the same distribution. In this case the order is of no consequence, any reordering should give an equally valid answer:

```{r}
z <- sample(c(norm1.data, norm2.data)) #permutation
x <- z[1:20]
y <- z[21:50]
(mean(x)-mean(y))/sqrt((19*var(x)+29*var(y))/49)
```

This is a perfectly legitimate value of T **IF** the null hypothesis is true.

Let's repeat this many times. In fact let's write a function that does it:

```{r}
perm.test <- function(x, y,  B = 1e4, Show=FALSE) {
  n <- length(x)
  m <- length(y)
  T0 <- (mean(x) - mean(y))/
      sqrt(((n-1)*var(x)+(m-1)*var(y))/(n+m-2))  
  xy <- c(x, y)
  T <- rep(0, B)
  for(i in 1:B) {
    z <- sample(xy)
    x <- z[1:n]
    y <- z[(n+1):(n+m)]
    T[i] <- (mean(x) - mean(y))/
      sqrt(((n-1)*var(x)+(m-1)*var(y))/(n+m-2))  
  }
  if(Show) {
    hist(T, 100, main="")
    abline(v=T0, lwd=2, col="blue")  
  }
  sum(abs(T)>abs(T0))/B
}
perm.test(norm1.data, norm2.data, Show=TRUE)
```

and we see that the value of T for the real data is in no way unusual.

 Let's do this again for some data where the means are indeed different:
 
```{r}
perm.test(x=rnorm(20, 10, 5),
          y=rnorm(30, 15, 5),
          Show = TRUE)
```
 
In our case we also know  that the F is a normal distribution. In this case there is of course a classic solution, the so-called *two-sample-t test*:

```{r}
t.test(norm1.data, norm2.data)$p.value
```

and notice that its p value is almost the same as the permutations tests!

How good a test is this? Let's find out:

```{r cache=TRUE}
pwr.sim <- function(mu2, n=20, m=30, B = 2500) {
  pvals <- matrix(0, B, 2)
  colnames(pvals) <- c("Permutation", "t test")
  for(i in 1:B) {
    x <- rnorm(n)
    y <- rnorm(m, mu2)
    pvals[i, 1] <- perm.test(x, y, B=2500)
    pvals[i, 2] <- t.test(x, y)$p.value
  }
  pvals
}
```

Let's do a whole power curve! This takes a while to run, though, so the result is saved as *pwr.tbl*

```{r eval=FALSE, echo=FALSE}
mu2 <- seq(0, 1, 0.05)
tmp <- matrix(0, 21, 2)
for(i in 1:21) 
  tmp[i, ] <-
  apply(pwr.sim(mu2[i]), 2, 
        function(x) {sum(x<0.05)})/2500
```

```{r, echo=FALSE, eval=FALSE}
do.sim <- function(mu2.rg, B=500) {
  mu2 <- seq(mu.rg[1], mu.rg[2], length=100)
  tmp <- matrix(0, 100, 2)
  for(i in 1:100) 
    tmp[i, ] <-
    apply(pwr.sim(mu2[i]), 2, 
        function(x) {sum(x<0.05)})/500
}
```

```{r, echo=FALSE}
pwr.tbl <-
structure(c(-1.2, -1.176, -1.152, -1.127, -1.103, -1.079, -1.055, 
-1.03, -1.006, -0.982, -0.958, -0.933, -0.909, -0.885, -0.861, 
-0.836, -0.812, -0.788, -0.764, -0.739, -0.715, -0.691, -0.667, 
-0.642, -0.618, -0.594, -0.57, -0.545, -0.521, -0.497, -0.473, 
-0.448, -0.424, -0.4, -0.376, -0.352, -0.327, -0.303, -0.279, 
-0.255, -0.23, -0.206, -0.182, -0.158, -0.133, -0.109, -0.085, 
-0.061, -0.036, -0.012, 0.012, 0.036, 0.061, 0.085, 0.109, 0.133, 
0.158, 0.182, 0.206, 0.23, 0.255, 0.279, 0.303, 0.327, 0.352, 
0.376, 0.4, 0.424, 0.448, 0.473, 0.497, 0.521, 0.545, 0.57, 0.594, 
0.618, 0.642, 0.667, 0.691, 0.715, 0.739, 0.764, 0.788, 0.812, 
0.836, 0.861, 0.885, 0.909, 0.933, 0.958, 0.982, 1.006, 1.03, 
1.055, 1.079, 1.103, 1.127, 1.152, 1.176, 1.2, 0.98, 0.979, 0.975, 
0.968, 0.965, 0.959, 0.947, 0.94, 0.924, 0.914, 0.893, 0.892, 
0.864, 0.848, 0.825, 0.81, 0.788, 0.767, 0.729, 0.698, 0.687, 
0.65, 0.611, 0.586, 0.55, 0.524, 0.481, 0.459, 0.424, 0.39, 0.362, 
0.322, 0.299, 0.281, 0.241, 0.222, 0.197, 0.178, 0.166, 0.141, 
0.132, 0.108, 0.093, 0.079, 0.07, 0.07, 0.06, 0.053, 0.051, 0.051, 
0.049, 0.048, 0.057, 0.058, 0.068, 0.075, 0.081, 0.097, 0.106, 
0.115, 0.144, 0.158, 0.175, 0.2, 0.22, 0.25, 0.273, 0.302, 0.33, 
0.357, 0.371, 0.428, 0.45, 0.483, 0.529, 0.556, 0.588, 0.618, 
0.657, 0.675, 0.712, 0.73, 0.76, 0.788, 0.813, 0.828, 0.856, 
0.852, 0.889, 0.897, 0.913, 0.931, 0.936, 0.942, 0.955, 0.963, 
0.973, 0.972, 0.975, 0.983, 0.981, 0.973, 0.975, 0.966, 0.959, 
0.942, 0.942, 0.95, 0.925, 0.92, 0.891, 0.879, 0.852, 0.841, 
0.843, 0.791, 0.806, 0.75, 0.729, 0.698, 0.674, 0.662, 0.622, 
0.555, 0.518, 0.493, 0.53, 0.458, 0.398, 0.38, 0.37, 0.335, 0.332, 
0.274, 0.264, 0.206, 0.19, 0.171, 0.167, 0.131, 0.118, 0.106, 
0.091, 0.088, 0.063, 0.056, 0.063, 0.047, 0.055, 0.054, 0.053, 
0.058, 0.042, 0.064, 0.068, 0.08, 0.089, 0.085, 0.102, 0.117, 
0.14, 0.146, 0.17, 0.207, 0.209, 0.22, 0.241, 0.307, 0.312, 0.347, 
0.389, 0.382, 0.438, 0.518, 0.52, 0.584, 0.603, 0.61, 0.628, 
0.682, 0.696, 0.731, 0.778, 0.788, 0.81, 0.81, 0.861, 0.882, 
0.891, 0.889, 0.927, 0.92, 0.94, 0.939, 0.959, 0.965, 0.973, 
0.974, 0.978, 0.982), .Dim = c(100L, 3L))

```

```{r}
df <- data.frame(x=rep(pwr.tbl[, 1], 2),
       y=c(pwr.tbl[, 2], pwr.tbl[, 3]),
       Method=rep(c("Permutation", "t test"), each=100))
ggplot(data=df, aes(x, y, color=Method)) +
  geom_line()
```

Can't tell the difference? In fact the two methods have just about the same power, even so one dependes strongly on the normal assumption, whereas the other one works without it.

This test was first discussed by Fisher in the 1930's, but until fairly recently it was not doable. Nowadays it should be considered the go-to test for this kind of situation.

`r hl()$hr()`
