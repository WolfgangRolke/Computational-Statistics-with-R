---
title: Principle Components Analysis
header-includes: \usepackage{color}
output:
  html_document: default
  pdf_document:
    fig_caption: no
---
<style>
table, th, td { text-align:right; }
th, td {padding: 10px;}
</style>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
whichcomp <- strsplit(getwd(),"/")[[1]][3]
load(paste0("c:/users/", whichcomp, "/Dropbox/teaching/Resma3/Resma3.RData"))
library(knitr)
opts_chunk$set(fig.width=6, fig.align = "center", 
      out.width = "70%", warning=FALSE, message=FALSE)
library(ggplot2)
library(grid)
```
`r hl()$basefontsize()`

As we saw before, highly correlated predictors can cause difficulties in a regression analysis. We will now study a way to deal with this.

The idea of principle components is this: find a few linear combinations of $x_1, .., x_k$ that explain the variation in y. That is, find 
$$
z_1=\sum \alpha_{i1} x_i \\
... \\
z_m=\sum \alpha_{ik} x_i \\
$$
and do a regression of y on $z_1, .., z_m$ instead of $x_1, .., x_k$.

This is a useful idea if the following happens:

-  m is much smaller than k  
-  $z_i$ and $z_j$ are uncorrelated  
-  $z_j=\sum \alpha_{ij} x_i$ can be interpreted in some way, that is we can understand the meaning of $z_j$

Using matrix notation we can write $Z=XA$ where X is the data matrix and A is the m by n matrix  of $\alpha$'s.

How should we choose A? The idea of principle components is as follows: Choose A such that

- the variables $z_1, .., z_m$ are uncorrelated (Z'Z is a diagonal matrix)  
- $z_1$ has the largest possible variance, then $z_2$ has the second largest (subject to cor($z_1$, $z_2$)=0), and so on.

So we want to find a matrix A such that Z'Z = (XA)'XA = A'(X'X)A = D

Now X'X is a symmetric k by k matrix. It could be singular but let's assume for the moment that it is not. Then using Linear Algebra it can be shown that the columns of A are the the eigenvectors of the matrix X'X. 

Let's see how this works on an artificial example.  We have a sample of 100 observations from a bivariate normal distribution with means (0,0), variances (1, 1) and correlation 0.9. First we plot x2 vs x1, then we find the matrix X'X and its eigenvectors (using the R function eigen). Next we find the new variables z1 and z2 as linear combinations of the eigenvectors and X. 

```{r}
library(mvtnorm)
x <- rmvnorm(100, mean = c(0, 0), 
             sigma = matrix(c(1, 0.9, 0.9, 1), 2, 2))
xyr<- range(x)
plot(x, xlab = "X1", ylab = "X2", xlim = xyr, ylim = xyr)
y <- t(x) %*% x
print(y)
E <- eigen(y)
print(E)
z1 <- E$vector[1, 1] * x[, 1] + E$vector[1, 2] * x[, 2]
z2 <- E$vector[2, 1] * x[, 1] + E$vector[2, 2] * x[, 2]
plot(z1, z2, xlab = "z1", ylab = "z2", ylim = range(z1))
```

Notice

-  z1 and z2 are uncorrelated (here, they are of course independent)  
-  the variance of z1 is much large than the variance of z2. 

There is another, geometric way to see what principle components are: again we draw the scatterplot of x2 vs. x1, but without the axes. The first principle component transformation is y=e1,1x1+e1,2x2. In the x2 vs. x1 plot this describes a line with slope -e1,1/e1,2 and going through the origin, which we add to the plot. We do the same with the second principle component transformation.
```{r}
plot(x, xlab = "x1", ylab = "x2", 
     xlim = xyr, ylim = xyr,  axes = F)
abline(0, -E$vector[1, 1]/E$vector[1, 2])
abline(0, -E$vector[2, 1]/E$vector[2, 2])
```

Now we can see that the transformation is really a change of coordinate system, from x1, x2 to z1, z2.

In practise we can use the R function *princomp* to carry out the calculations for us. 

```{r}
pc <- princomp(x)
print(summary(pc))
```

we see that the first principle component explains about 95% of the variation in (x1, x2). 

One of the components of the pc object is called "loadings" 

```{r}
pc$loadings
```

and we see that these are just the eigenvectors.

#### **Example**: Scores on math tests

 consider the data in testscores. This is artificial data supposed to be the test scores of 25 mathematics graduate students in their qualifying exams. The differential geometry and complex analysis exams were closed book whereas the others were open book.
 
```{r}
testscores
test.pc <- princomp(testscores)
summary(test.pc, loadings = TRUE)

```
 
Looking at the summary we see that the first pc accounts for 82% of the variation in the data, and the first two account for 90%.

Let's have a look at the loadings: the first one is (0.6, 0.36, 0.3, 0.39, 0.52) and amounts to an average over all the exams. The second one is (-0.67, -0.25, 0.21, 0.34, 0.57). Notice that here the first two are negative and the others are positive. But the first two were the closed book exams and the others were open book!

#### **Example**: States

The data set *state.x77* has info of the 50 states of the United States of America.

'Population': population estimate as of July 1, 1975  
'Income': per capita income (1974)  
'Illiteracy': illiteracy (1970, percent of population)  
'Life Exp': life expectancy in years (1969-71)  
'Murder': murder and non-negligent manslaughter rate per 100,000 population (1976)  
'HS Grad': percent high-school graduates (1970)  
'Frost': mean number of days with minimum temperature below freezing (1931-1960) in capital or large city  
'Area': land area in square miles  


Source: U.S. Department of Commerce, Bureau of the Census (1977) Statistical Abstract of the United States.

```{r}
head(state.x77)
```

```{r}
state.pc <- princomp(state.x77, cor = T)
summary(state.pc, loading = TRUE)
```

It appears that the first PC contrasts "good" variables such as income and life expectancy with bad ones such as murder and illiteracy. This explains about 45% of the variation. The second PC contrasts 'Frost' with all the other variables. It accounts for an additional 20% but it seems difficult to understand exactly what that means.

One important question is how many PCs are needed to "reasonably" explain the data? One useful graph here is the screeplot, given in 

```{r}
plot(state.pc)
```

It is a simple barchart of the the variation explained by each PC. One popular method is to include enough PCs to cover at least 90% of the variation. In the states data this means 5, which seems a bit much.