---
title: Logistic Regression - General Linear Models
                 \usepackage{float}
header-includes: \usepackage{color}
output:
  pdf_document:
    fig_caption: no
  html_document: default
---
<style>
table, th, td { text-align:right; }
th, td {padding: 10px;}
</style>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
whichcomp <- strsplit(getwd(),"/")[[1]][3]
load(paste0("c:/users/", whichcomp, "/Dropbox/teaching/Resma3/Resma3.RData"))
library(knitr)
opts_chunk$set(fig.width=6, fig.align = "center", 
      out.width = "70%", warning=FALSE, message=FALSE)
library(ggplot2)
library(grid)
```
`r hl()$basefontsize()`

In all the examples so far we always had a quantitative response. In this section we will study the case of a categorical response.

#### **Example**: Challenger shuttle disaster

We begin with a very famous dataset from the Challenger shuttle disaster. On Jan 28, 1986, at 11.38 am EST, the space shuttle challenger was launched from Cape Canaveral, Florida. The mission ended 73 seconds later when the Challenger exploded. All 7 crew members were killed.

[Challenger Disaster Movie](http://academic.uprm.edu/wrolke/esma6665/challenger.lg.mp4)

What happened?

Hot propellant gases flew past the aft joint of the right solid rocket booster, burning through two rubber O-rings.
An investigation ensued into the reliability of the shuttle's propulsion system. The explosion was eventually traced to the failure of one of the three field joints on one of the two solid booster rockets. Each of these six field joints includes two O-rings, designated as primary and secondary, which fail when phenomena called erosion and blowby both occur.

The night before the launch a decision had to be made regarding launch safety. The discussion among engineers and managers leading to this decision included concern that the probability of failure of the O-rings depended on the temperature t at launch, which was forecase to be 31 degrees F. There are strong engineering reasons based on the composition of O-rings to support the judgment that failure probability may rise monotonically as temperature drops.

The discussion centered on the following data from the previous 23 shuttle launches: 

```{r}
kable(head(shuttle))
```

```{r}
ggplot(data=shuttle, aes(Temp, NumFail)) +
  geom_point() 
```

there seems to be a tendency for failures at lower temperatures.

The variable Failure is an indicator of failure or not:

```{r}
plt <- ggplot(data=shuttle, aes(Temp, Failure)) +
  geom_jitter(height = 0) 
plt
```

Again we want to use regression to study the relationship between temperature and failure. But now failure is categorical, and so the x and the y variable are no longer measured in the same way.

The way to connect them is to predict the probability of failure, which again is a quantitative variable. This is done as follows: we have responses $y_1, .., y_n$ which acn be modeled as $Y_i \sim Ber(\pi_i)$. The $\pi$ are related to the predictor variable $x$ via the *link function*

$$
\log \left(\frac{p}{1-p} \right) =\alpha + \beta x
$$

this is called the *logit* link function. There are others as well.

We can invert the logit function:

$$
\pi(x)=\frac{e^{\alpha + \beta x}}{1+e^{\alpha + \beta x}}
$$

notice that this rules out $\pi(x)=0$ or $1$. There are other link functions that don't do that.

How do fit such a model, that is find $\alpha$ and $\beta$? For linear regression we used the method of least squares, which was possible because we could directly compare x and y. This is not the case here because p and x have different forms, which is why we needed a link function. Instead one uses maximum likelihood for the estimation. In R this is done using the command *glm* with the argument *family=binomial*:

```{r}
fit <- glm(Failure~Temp, 
           family=binomial,
           data=shuttle)
fit
```

Let's see what this looks like


```{r}
x <- seq(30, 80, 1)
df <- data.frame(x=x, 
                 y=predict(fit, data.frame(Temp=x), 
                                type="response"))
plt +
  geom_line(data=df, aes(x, y),
            color="blue", size=1.2)
```

we see that at the expected launch temperature of 32F the failure probability is 1.

What would be a $95\%$ confidence interval for the probability at 32F?

```{r}
tmp <- predict(fit, data.frame(Temp=32), 
     type="response", se.fit=TRUE)
round(tmp$fit +c(-1, 1)*qnorm(0.975)*tmp$se.fit, 3)
```

but there is something silly about this interval: it goes beyond 1! This is a consequence of using normal theory intervals. Here is a better solution:

```{r}
tmp <- predict(fit, data.frame(Temp=32), 
     type="link", se.fit=TRUE)
e <- tmp$fit
r <- tmp$se.fit
e
r
cr <- qnorm(0.975)
round(c(exp(e-cr*r)/(1+exp(e-cr*r)),
        exp(e+cr*r)/(1+exp(e+cr*r))), 3)
```

but this has a much lower (likely to low) lower limit.

#### Warp Breaks

The dataset gives the results of an experiment to determine the effect of wool type (A or B) and tension (low, medium or high) on the number of warp breaks per loom. Data was collected for nine looms for each combination of settings. 

```{r}
head(warpbreaks)
```

we want to build a model relating the wool type and tension to the number of breaks.

What distribution might be appropriate for *breaks*? First, let's have a look at them:

```{r}
bw <- diff(range(warpbreaks$breaks))/20 
ggplot(warpbreaks, aes(x=breaks)) +
  geom_histogram(color = "black", 
    fill = "white", 
    binwidth = bw) + 
    labs(x = "x", y = "Breaks")
```    

our data is counts with a bit of skew to the right. This is typical for data from a *Poisson* distribution.

Here is another argument in favor of a Poisson: Each loom could be considered as a series of small intervals. We then would have a large number of such intervals, each of which has a small probability of a break. The total number of breaks would be the sum of the breaks in each interval, and therefore would be Binomial. But in this case the Poisson approximation to the Binomial would be very good.

Again we want to use regression to relate type and tension to breaks. In the case of a Poisson response variable the link function is given by the logarithm.

```{r}
fit <- glm(breaks~wool*tension, 
           data=warpbreaks, 
           family=poisson) 
summary(fit)
```

and we see that all terms except one interaction term are stat. significant.

`r hl()$hr()`

Let's do our own little study of Poisson regression. First we generate some data:

```{r}
x <- 1:100/50
df <- data.frame(x=x, y=rpois(100, 10*x))
plt <- ggplot(data=df, aes(x, y)) +
  geom_point() 
plt
fit <- glm(y~x, 
           data=df, 
           family=poisson) 
summary(fit)
df1 <- df
df1$y <- predict(fit, type="response")
plt + 
  geom_line(data=df1, aes(x, y), color="blue", size=1.2)
```
 and that looks quite good!
